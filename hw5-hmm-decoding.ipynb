{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should not modify code in this cell\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "# Get numsents POS-tagged sentences from the treebank corpus\n",
    "def get_pos_data(numsents):\n",
    "\n",
    "    # Extract required number of sentences\n",
    "    sentences = treebank.tagged_sents()[:numsents]\n",
    "\n",
    "    # Initialize\n",
    "    sequences = []\n",
    "    symbols = set()\n",
    "    tag_set = set()\n",
    "    \n",
    "    # Go over each extracted sentence ...\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            word, tag = sentence[i]\n",
    "            word = word.lower()  # normalize case\n",
    "            symbols.add(word)    # add this word\n",
    "            tag_set.add(tag)\n",
    "            sentence[i] = (word, tag)  # store tagged token\n",
    "        sequences.append(sentence)\n",
    "\n",
    "    # Return sequences, the list of tags and all the words that we saw\n",
    "    return sequences, list(tag_set), list(symbols)\n",
    "\n",
    "# Train the transition and emission probabilities\n",
    "def train():\n",
    "    print('Training HMM...')\n",
    "\n",
    "    # Use the first 5000 sentences from treebank corpus\n",
    "    labelled_sequences, states, symbols = get_pos_data(5000)\n",
    "    \n",
    "    # Define the estimator to be used for probability computation\n",
    "    estimator = lambda fd, bins: nltk.LidstoneProbDist(fd, 0.1, bins)\n",
    "    \n",
    "    # count occurences of starting states, transitions out of each state\n",
    "    # and output symbols observed in each state\n",
    "    freq_starts = nltk.FreqDist()\n",
    "    freq_transitions = nltk.ConditionalFreqDist()\n",
    "    freq_emissions = nltk.ConditionalFreqDist()\n",
    "    for sequence in labelled_sequences:\n",
    "        lasts = None\n",
    "        for token in sequence:\n",
    "            state = token[1]\n",
    "            symbol = token[0]\n",
    "            if lasts == None:\n",
    "                freq_starts[state] += 1\n",
    "            else:\n",
    "                freq_transitions[lasts][state] += 1\n",
    "            freq_emissions[state][symbol] += 1\n",
    "            lasts = state\n",
    "\n",
    "            # update the state and symbol lists\n",
    "            if state not in states:\n",
    "                states.append(state)\n",
    "            if symbol not in symbols:\n",
    "                symbols.append(symbol)\n",
    "\n",
    "    # create probability distributions (with smoothing)\n",
    "    N = len(states)\n",
    "    starts = estimator(freq_starts, N)\n",
    "    transitions = nltk.ConditionalProbDist(freq_transitions, estimator, N)\n",
    "    emissions = nltk.ConditionalProbDist(freq_emissions, estimator, len(symbols))\n",
    "                               \n",
    "    print('Training Complete') \n",
    "    \n",
    "    # Return the transition and emissions probabilities along with \n",
    "    # the list of all the states and output symbols\n",
    "    return starts, transitions, emissions, states, symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM...\n",
      "Training Complete\n",
      "['``', '-NONE-']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['hello', 'world'], ['``', '-NONE-'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import zeros, array, float32, int16, argmax\n",
    "from math import log, exp\n",
    "from nltk import re\n",
    "import numpy\n",
    "\n",
    "# call the train function (it will take some time)\n",
    "priors, transitions, emissions, states, symbols = train()\n",
    "# suggestion: inspect these five variables and the code in train() \n",
    "# to get a sense of the data and data structures\n",
    "\n",
    "# write your viterbi code here\n",
    "def viterbi(sentences):\n",
    "    sent = re.compile(r\"[\\w']+|[(){}.,!?;]\")\n",
    "    observations = sent.findall(sentences)\n",
    "    \n",
    "    N = len(states)\n",
    "    T = len(observations)\n",
    "\n",
    "    matrix = numpy.zeros(shape=(N, T), dtype=float32)\n",
    "    backpointer = {}\n",
    "    \n",
    "    for x in range(0,N):\n",
    "        matrix[x][0] = priors.logprob(states[x]) + emissions[states[x]].logprob(observations[0])\n",
    "        backpointer[x,0] = 0\n",
    "        \n",
    "    for y in range(1,T):\n",
    "        for x in range(0,N):\n",
    "            prev_state = None\n",
    "            for z in range(0,N):\n",
    "                current_prob = matrix[z][y-1]\n",
    "                + transitions[states[z]].logprob(states[x])\n",
    "                + emissions[states[x]].logprob(observations[y])\n",
    "                if not prev_state or current_prob > prev_state[0]:\n",
    "                    prev_state = (current_prob, states[z])\n",
    "            matrix[x][y] = prev_state[0]\n",
    "            backpointer[states[x],y] = prev_state[1]\n",
    "    \n",
    "    temp = None\n",
    "    for n in range(0,N):\n",
    "        prob = matrix[n,T-1]\n",
    "        if not temp or prob > temp[0]:\n",
    "            temp = (prob, states[n])\n",
    "    \n",
    "    curr_val = temp[1]\n",
    "    best_path = [curr_val]\n",
    "    for i in range (T-1, 0, -1):\n",
    "        last_val = backpointer[curr_val,i]\n",
    "        best_path.append(last_val)\n",
    "        current_val = last_val\n",
    "    \n",
    "    best_path.reverse()\n",
    "    print(best_path)\n",
    "    return observations, best_path\n",
    "                           \n",
    "\n",
    "sentence = 'hello world' \n",
    "viterbi(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training HMM...\n",
      "Training Complete\n",
      "['DT', '-NONE-']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['hello', 'world'], ['DT', '-NONE-'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import zeros, array, float32, int16, argmax\n",
    "from math import log, exp\n",
    "from nltk import re\n",
    "# call the train function (it will take some time)\n",
    "priors, transitions, emissions, states, symbols = train()\n",
    "# suggestion: inspect these five variables and the code in train() \n",
    "# to get a sense of the data and data structures\n",
    "\n",
    "\n",
    "# write your viterbi code here  \n",
    "#pass in observed sentences \n",
    "\n",
    "def viterbi(string_words):\n",
    "    sentence = re.compile(r\"[\\w']+|[(){}.,!?;]\")\n",
    "    observations = sentence.findall(string_words)\n",
    "    \n",
    "    N = len(states)\n",
    "    T = len(observations)\n",
    "    backpointer = {}\n",
    "    matrix = numpy.zeros(shape=(N,T), dtype=float32)\n",
    "    \n",
    "    for x in range (0,N):\n",
    "        matrix[x][0] = priors.logprob(states[x])\n",
    "        + emissions[states[x]].logprob(observations[0])\n",
    "        \n",
    "        backpointer[x,0] = 0\n",
    "        \n",
    "    for y in range (1,T):\n",
    "        for x in range (0,N):\n",
    "            prev_state = None\n",
    "            for z in range (0,N):\n",
    "                current_prob = matrix[z][y-1]\n",
    "                + transitions[states[z]].logprob(states[x])\n",
    "                + emissions[states[x]].logprob(observations[y])\n",
    "                if not prev_state or current_prob > prev_state[0]:\n",
    "                    prev_state = (current_prob, states[z])\n",
    "            matrix[x][y] = prev_state[0]\n",
    "            backpointer[states[x],y] = prev_state[1]\n",
    "            \n",
    "    temp = None\n",
    "    for n in range(0,N):\n",
    "        current_val = matrix[n, T-1]\n",
    "        if not temp or current_val > temp[0]:\n",
    "            temp = (current_val, states[n])\n",
    "    current = temp[1]\n",
    "    bestpath = [current]\n",
    "    for i in range(T-1, 0, -1):\n",
    "        last_val = backpointer[current,i]\n",
    "        bestpath.append(last_val)\n",
    "        current = last_val\n",
    "    bestpath.reverse()\n",
    "    print(bestpath)\n",
    "    return observations, bestpath\n",
    "    \n",
    "    \n",
    "    \n",
    "s = 'hello world'\n",
    "viterbi(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open test-sentences\n",
    "testSentenceFile = open('test-sentences.txt')\n",
    "tagViterbi(testSentenceFile) # you need to define a tagViterbi function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eventually, open the hw-sentences\n",
    "hwSentenceFile = open('hw-sentences.txt')\n",
    "tagViterbi(hwSentenceFile) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
